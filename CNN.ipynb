{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Convolution2D,MaxPool2D,Flatten,Dense,GlobalAveragePooling2D,Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import h5py\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2682 images belonging to 7 classes.\n",
      "Found 676 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'train',\n",
    "        target_size=(128, 128),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'valid',\n",
    "        target_size=(128, 128),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier=Sequential()\n",
    "classifier.add(Convolution2D(filters = 64,kernel_size = (3,3),input_shape=(128, 128, 3),\n",
    "                             activation='relu', data_format=\"channels_last\"))\n",
    "\n",
    "classifier.add(Convolution2D(filters = 128,kernel_size = (3,3), activation='relu', data_format=\"channels_last\"))\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "classifier.add(Convolution2D(filters = 256,kernel_size = (3,3), activation='relu', data_format=\"channels_last\"))\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "classifier.add(Flatten('channels_last'))\n",
    "\n",
    "\n",
    "classifier.add(Dense(output_dim = 500, activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "\n",
    "classifier.add(Dense(output_dim = 400, activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "\n",
    "classifier.add(Dense(output_dim = 300, activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "\n",
    "classifier.add(Dense(output_dim = train_generator.num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "classifier.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1000/1000 [==============================] - 6171s 6s/step - loss: 1.7208 - categorical_accuracy: 0.2829 - val_loss: 1.5322 - val_categorical_accuracy: 0.3891\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 6117s 6s/step - loss: 1.3694 - categorical_accuracy: 0.4516 - val_loss: 1.2699 - val_categorical_accuracy: 0.4789\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 7591s 8s/step - loss: 1.0120 - categorical_accuracy: 0.6246 - val_loss: 1.5637 - val_categorical_accuracy: 0.4869\n"
     ]
    }
   ],
   "source": [
    "history = classifier.fit_generator(train_generator,epochs=3,\n",
    "                         steps_per_epoch = 1000,\n",
    "                         validation_data=validation_generator,\n",
    "                         validation_steps=300,\n",
    "                         use_multiprocessing=True,workers=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = classifier.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "classifier.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "22/22 [==============================] - 30s 1s/step\n",
      "categorical_accuracy: 48.67%\n"
     ]
    }
   ],
   "source": [
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "loaded_model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "score = loaded_model.evaluate(validation_generator)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = loaded_model.predict(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "for pred in predict:\n",
    "    pred_list.append(np.argmax(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.21      0.22       113\n",
      "           1       0.04      0.02      0.03        53\n",
      "           2       0.16      0.21      0.18        87\n",
      "           3       0.27      0.18      0.22       125\n",
      "           4       0.14      0.17      0.15        89\n",
      "           5       0.18      0.25      0.21       113\n",
      "           6       0.16      0.12      0.14        96\n",
      "\n",
      "    accuracy                           0.18       676\n",
      "   macro avg       0.17      0.17      0.16       676\n",
      "weighted avg       0.18      0.18      0.18       676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_generator.labels,pred_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
